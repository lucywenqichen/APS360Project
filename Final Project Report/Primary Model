import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
import matplotlib.pyplot as plt
import random

from torchvision import transforms
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
from tqdm import tqdm
from skimage.color import lab2rgb
from skimage import color, io
from collections import Counter

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # Fixes a bug with PyTorch and MKL

from google.colab import drive
drive.mount('/content/drive')

class ImageDataset(Dataset):
    def __init__(self, tensor_dir, ab_grid, tensor_files, transform=None):
        self.tensor_dir = tensor_dir
        self.ab_grid = ab_grid
        self.transform = transform
        self.tensor_files = tensor_files

    def __len__(self):
        return len(self.tensor_files)

    def __getitem__(self, idx):
        tensor_path = os.path.join(self.tensor_dir, self.tensor_files[idx])
        try:
            tensor = torch.load(tensor_path)
        except Exception as e:
            print(f"Skipping file {tensor_path}: {e}")
            raise

        L = tensor['L']
        ab = tensor['ab']

        # Simple color augmentation: add small random noise to ab channels
        if self.transform is not None:
            noise = torch.randn_like(ab) * 0.05  # Small noise
            ab = ab + noise
            ab = torch.clamp(ab, -1, 1)  # Keep in valid range

        labels = tensor['label']

        return L.float(), ab.float(), labels.long()

class ColorizationNet(nn.Module):
    def __init__(self, num_classes):
        super(ColorizationNet, self).__init__()
        # Encoder:
        self.enc_conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1) # (1, 1024, 1024) -> (64, 1024, 1024)
        self.enc_bn1 = nn.BatchNorm2d(64)

        self.enc_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # (64, 1024, 1024) -> (128, 512, 512)
        self.enc_bn2 = nn.BatchNorm2d(128)

        self.enc_conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # (128, 512, 512) -> (256, 256, 256)
        self.enc_bn3 = nn.BatchNorm2d(256)

        self.enc_conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1) # (256, 256, 256) -> (512, 128, 128)
        self.enc_bn4 = nn.BatchNorm2d(512)

        # skip connections
        self.skip1 = nn.Conv2d(256, 256, kernel_size = 1)
        self.skip2 = nn.Conv2d(128, 128, kernel_size = 1)
        self.skip3 = nn.Conv2d(64, 64, kernel_size = 1)

        # Decoder:
        self.dec_conv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1) # (512, 128, 128) -> (256, 256, 256)
        self.dec_bn1 = nn.BatchNorm2d(256)
        self.dec_conv_skip1 = nn.Conv2d(512, 256, kernel_size=1)
        self.dec_bn_skip1 = nn.BatchNorm2d(256)

        self.dec_conv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1) # (256, 256, 256) -> (128, 512, 512)
        self.dec_bn2 = nn.BatchNorm2d(128)
        self.dec_conv_skip2 = nn.Conv2d(256, 128, kernel_size=1)
        self.dec_bn_skip2 = nn.BatchNorm2d(128)

        self.dec_conv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1) # (128, 512, 512) -> (64, 1024, 1024)
        self.dec_bn3 = nn.BatchNorm2d(64)
        self.dec_conv_skip3 = nn.Conv2d(128, 64, kernel_size=1)
        self.dec_bn_skip3 = nn.BatchNorm2d(64)

        # predict softmax oover quantized color bins per pixel
        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1) # (64, 1024, 1024) -> (2, 1024, 1024)

    def forward(self, x):
        # Encoder:
        x1 = F.relu(self.enc_bn1(self.enc_conv1(x)))
        x2 = F.relu(self.enc_bn2(self.enc_conv2(x1)))
        x3 = F.relu(self.enc_bn3(self.enc_conv3(x2)))
        x4 = F.relu(self.enc_bn4(self.enc_conv4(x3)))

        # Decoder:
        y1 = F.relu(self.dec_bn1(self.dec_conv1(x4)))
        y1 = torch.cat([y1, x3], dim=1)
        y1 = F.relu(self.dec_bn_skip1(self.dec_conv_skip1(y1)))

        y2 = F.relu(self.dec_bn2(self.dec_conv2(y1)))
        y2 = torch.cat([y2, x2], dim=1)
        y2 = F.relu(self.dec_bn_skip2(self.dec_conv_skip2(y2)))

        y3 = F.relu(self.dec_bn3(self.dec_conv3(y2)))
        y3 = torch.cat([y3, x1], dim=1)
        y3 = F.relu(self.dec_bn_skip3(self.dec_conv_skip3(y3)))

        # Output layer:
        out = self.final_conv(y3)
        return out

class EarlyStop:
    def __init__(self, patience=5):
        self.patience = patience
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, val_loss):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

def create_ab_grid(num_bins = 18, ab_range = 110):
    a_vals = np.linspace(-ab_range, ab_range, num_bins)
    b_vals = np.linspace(-ab_range, ab_range, num_bins)

    aa, bb = np.meshgrid(a_vals, b_vals)
    ab_grid = np.stack([aa.flatten(), bb.flatten()], axis=1)
    return ab_grid

def ab_to_classes(ab, ab_grid):
    ab = ab.float()  # Ensure ab is float32
    ab = ab.to(ab_grid.device)

    ab_scaled = ab * 110  # Scale back to original LAB range

    ab_flat = ab_scaled.permute(1, 2, 0).reshape(-1, 2)
    dists = torch.cdist(ab_flat.unsqueeze(0), ab_grid.unsqueeze(0)).squeeze(0)
    labels = dists.argmin(dim=1)
    return labels.reshape(ab.shape[1], ab.shape[2])

def class_to_ab(labels, ab_grid):
    flat_labels = labels.view(-1).cpu().numpy()
    ab = ab_grid[flat_labels]
    ab = ab.reshape(labels.shape[0], labels.shape[1], labels.shape[2], 2)
    ab = ab.permute(0, 3, 1, 2)  # [batch, 2, H, W]
    return ab.float() / 110

def preprocess_image(image_path, save_path, ab_grid):
    image = io.imread(image_path)
    lab = color.rgb2lab(image)
    L = lab[:, :, 0]
    ab = lab[:, :, 1:]

    L = torch.from_numpy(L).unsqueeze(0) / 100.0  # (1, H, W)
    ab = torch.from_numpy(ab).permute(2, 0, 1) / 110.0  # (2, H, W)

    label = ab_to_classes(ab, ab_grid)

    torch.save({'L': L, 'ab': ab, 'label': label}, save_path)

def class_weights(dataset, num_classes):
    counter = Counter()
    for _, _, labels in tqdm(dataset, desc="Computing class weights", unit="sample"):
        counter.update(labels.cpu().numpy().flatten().tolist())

    total = sum(counter.values())
    freqs = np.array([counter.get(i, 0)/total for i in range(num_classes)])

    # Use sqrt to reduce impact of very rare colors but still balance
    weights = 1/np.sqrt(freqs + 1e-6)  # sqrt instead of direct inverse
    weights = np.clip(weights, 0.1, 10.0)  # Prevent extreme weights
    weights = torch.tensor(weights, dtype=torch.float32)
    return weights/weights.sum() * num_classes

def accuracy(gt_ab, pred_ab, ab_grid, distance_threshold=0.5):
    # distance threshold is the maximum distance between a predicted color and a ground truth color
    pred_flat = pred_ab.permute(0, 2, 3, 1).reshape(-1, 2)  # [batch, H, W, 2] -> [batch*H*W, 2]
    gt_flat = gt_ab.permute(0, 2, 3, 1).reshape(-1, 2)      # [batch, H, W, 2] -> [batch*H*W, 2]

    distances = torch.norm(pred_flat - gt_flat, dim=1)
    correct = (distances <= distance_threshold).sum().item()
    total = len(distances)

    return correct, total

def total_accuracy(model, device, loader, ab_grid, distance_threshold=0.5):
    # distance threshold is the maximum distance between a predicted color and a ground truth color
    model.eval()
    t_correct = 0
    t_pixels = 0

    with torch.no_grad():
        for L, gt_ab, labels in loader:
            L = L.to(device)
            gt_ab = gt_ab.to(device)

            output = model(L)
            preds = output.argmax(dim=1)

            pred_ab = class_to_ab(preds, ab_grid).to(device)

            correct, total = accuracy(gt_ab, pred_ab, ab_grid, distance_threshold)
            t_correct += correct
            t_pixels += total

    return t_correct / t_pixels if t_pixels > 0 else 0

def class_accuracy(model, device, loader):
    model.eval()
    t_correct = 0
    t_pixels = 0
    with torch.no_grad():
        for inputs, _, labels in loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)  # [B, C, H, W]
            preds = outputs.argmax(dim=1)  # [B, H, W]
            t_correct += (preds == labels).sum().item()
            t_pixels += labels.numel()
    return t_correct / t_pixels if t_pixels > 0 else 0

def train(model, device, ab_grid, train_loader, val_loader, weights=None, num_epochs = 20, lr = 0.001, accuracy_interval = 5, op = 'Adam', wd=1e-4):
    model.to(device)
    if weights is None:
          weights = class_weights(train_loader.dataset, num_classes=len(ab_grid)).to(device)

    # Use label smoothing to encourage more diverse predictions
    criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)

    if op == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    elif op == 'AdamW':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    elif op == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)
    else:
        raise ValueError(f"Unsupported optimizer: {op}")

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    early_stop = EarlyStop(patience=10)

    iters, train_losses, val_losses, train_acc, val_acc, train_class_acc, val_class_acc = [], [], [], [], [], [], []
    n = 0
    best_val_acc = 0

    for epoch in tqdm(range(num_epochs)):
        model.train()
        epoch_loss = 0
        for input, _, labels in train_loader:
            input, labels = input.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            output = model(input)
            loss = criterion(output, labels)
            loss.backward()
            optimizer.step()

            iters.append(n)
            train_losses.append(loss.item())
            epoch_loss += loss.item()
            n += 1

        model.eval()
        val_loss_total = 0
        with torch.no_grad():
            for input, _, labels in val_loader:
                input, labels = input.to(device, non_blocking=True), labels.to(device, non_blocking=True)
                output = model(input)
                loss = criterion(output, labels)
                val_loss_total += loss.item()

        val_loss = val_loss_total / len(val_loader)
        val_losses.append(val_loss)

        scheduler.step(val_loss)

        if (epoch + 1) % accuracy_interval == 0 or epoch == num_epochs - 1:
            train_acc.append(total_accuracy(model, device, train_loader, ab_grid))
            val_acc.append(total_accuracy(model, device, val_loader, ab_grid))
            train_class_acc.append(class_accuracy(model, device, train_loader))
            val_class_acc.append(class_accuracy(model, device, val_loader))
        else:
            train_acc.append(train_acc[-1] if train_acc else 0)
            val_acc.append(val_acc[-1] if val_acc else 0)
            train_class_acc.append(train_class_acc[-1] if train_class_acc else 0)
            val_class_acc.append(val_class_acc[-1] if val_class_acc else 0)

        print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, "
            f"Train Accuracy (dist): {train_acc[-1]:.4f}, Validation Accuracy (dist): {val_acc[-1]:.4f}, "
            f"Train Class Accuracy: {train_class_acc[-1]:.4f}, Validation Class Accuracy: {val_class_acc[-1]:.4f}")

        if val_acc[-1] > best_val_acc:
            best_val_acc = val_acc[-1]
            torch.save(model.state_dict(), f'/content/drive/MyDrive/model_{op}_lr{lr:.0e}.pth')
            print(f"Saved model at epoch {epoch+1} with validation accuracy: {best_val_acc:.4f}")

        early_stop(val_loss)
        if early_stop.early_stop:
            print("Early stopping")
            break

    print(f"Training complete")
    max_val_epoch = val_acc.index(best_val_acc)
    print(f"Best validation accuracy: {best_val_acc:.4f} at epoch {max_val_epoch}")
    print(f"Corresponding training accuracy: {train_acc[max_val_epoch]:.4f}")

    plot(iters, train_losses, val_losses, num_epochs, train_acc, val_acc)

def plot(iters, train_losses, val_losses, num_epochs, train_acc, val_acc):
    # plot loss and accuracy
    plt.figure(figsize=(18, 5))

    plt.subplot(1, 3, 1)
    plt.plot(iters, train_losses, label="Training Loss")
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.title("Training Loss")

    plt.subplot(1, 3, 2)
    plt.plot(range(num_epochs), val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Validation Loss")

    plt.subplot(1, 3, 3)
    plt.plot(range(num_epochs), train_acc, label="Training Accuracy")
    plt.plot(range(num_epochs), val_acc, label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Training and Validation Accuracy")
    plt.legend()

    plt.tight_layout()
    plt.show()

def vis_class_to_ab(labels, ab_grid_np):
    flat_labels = labels.view(-1).cpu().numpy()
    ab = ab_grid_np[flat_labels]

    # Handle both 2D [H, W] and 3D [batch, H, W] tensors
    if len(labels.shape) == 2:  # Single image [H, W]
        ab = ab.reshape(labels.shape[0], labels.shape[1], 2)
        ab = torch.from_numpy(ab).permute(2, 0, 1)  # [2, H, W]
    else:  # Batch of images [batch, H, W]
        ab = ab.reshape(labels.shape[0], labels.shape[1], labels.shape[2], 2)
        ab = torch.from_numpy(ab).permute(0, 3, 1, 2)  # [batch, 2, H, W]

    # ab_grid values are in range [-110, 110], normalize to match preprocessing (/128)
    return ab.float() / 110.0

def lab_to_rgb(L, ab):
    L = L[0].cpu().numpy() * 100  # L ∈ [0, 1] → [0, 100]
    ab = ab.cpu().numpy() * 110   # ab ∈ [-1, 1] → [-128, 128] (original LAB range)

    lab = np.concatenate((L[np.newaxis, :, :], ab), axis=0).transpose(1, 2, 0)
    rgb = color.lab2rgb(lab)
    return np.clip(rgb, 0, 1)

def visualize_predictions(model, dataloader, device, ab_grid_np, num_images=5):
    """
    Displays grayscale input, model output, and ground truth colorization side-by-side.
    """
    model.eval()
    images_shown = 0

    with torch.no_grad():
        for batch in dataloader:
            L, ab_true, labels = batch
            ab_true = ab_true.to(device)
            L, labels = L.to(device), labels.to(device)
            logits = model(L)
            preds = torch.argmax(logits, dim=1)

            batch_size = L.size(0)
            for i in range(batch_size):
                if images_shown >= num_images:
                    return

                input_L = L[i]
                pred_labels = preds[i]
                gt_labels = labels[i]

                pred_ab = vis_class_to_ab(pred_labels, ab_grid_np)
                gt_ab = ab_true[i]

                input_rgb = lab_to_rgb(input_L, torch.zeros_like(pred_ab))
                pred_rgb = lab_to_rgb(input_L, pred_ab)
                gt_rgb = lab_to_rgb(input_L, gt_ab)

                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                axes[0].imshow(input_rgb)
                axes[0].set_title("Input (Grayscale)")
                axes[0].axis('off')

                axes[1].imshow(pred_rgb)
                axes[1].set_title("Model Output")
                axes[1].axis('off')

                axes[2].imshow(gt_rgb)
                axes[2].set_title("Ground Truth")
                axes[2].axis('off')

                plt.tight_layout()
                plt.show()

                images_shown += 1

def analyze_color_distribution(dataset, ab_grid, num_samples=100):
    """Analyze the color distribution in the dataset"""
    color_counts = torch.zeros(len(ab_grid))

    for i, (_, _, labels) in enumerate(dataset):
        if i >= num_samples:
            break
        unique, counts = torch.unique(labels, return_counts=True)
        for idx, count in zip(unique, counts):
            color_counts[idx] += count

    # Show top 10 most common colors
    top_colors = torch.topk(color_counts, 10)
    print("Top 10 most common color bins:")
    for i, (count, idx) in enumerate(zip(top_colors.values, top_colors.indices)):
        color = ab_grid[idx]
        print(f"{i+1}: Color bin {idx.item()}: a={color[0]:.1f}, b={color[1]:.1f}, count={count.item()}")

    return color_counts

def get_subset(tensor_folder, num_files, seed=42):
    all_tensor_files = [f for f in os.listdir(tensor_folder) if f.lower().endswith('.pt') and not f.startswith('.')]
    random.seed(42)
    random.shuffle(all_tensor_files)
    subset_tensor = all_tensor_files[:num_files]

    return subset_tensor

if __name__ == "__main__":
  hps = [[0.0001, 'Adam', 8, 0.001]]
  num_bins = [16]
  ne = 20
  
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  print(device)
  
  for num_bin in num_bins:
      print(f"Training Model with num_bins: {num_bin}")
      ab_grid_np = create_ab_grid(num_bins=num_bin, ab_range=110)
      ab_grid = torch.from_numpy(ab_grid_np).float()
  
      image_folder = "/content/drive/MyDrive/APS360Project/ProjectData/primarymodel-mishal/RGB_images"
      tensor_folder = "/content/drive/MyDrive/APS360Project/ProjectData/primarymodel-mishal/LAB_tensors"
  
      num_files = 4000
      tensors = get_subset(tensor_folder, num_files)
  
      train_size = int(0.6 * len(tensors))
      val_size = int(0.2 * len(tensors))
      test_size = len(tensors) - train_size - val_size
  
      train_tensors = tensors[:train_size]
      val_tensors = tensors[train_size:train_size + val_size]
      test_tensors = tensors[train_size + val_size:]
  
      print(f"Dataset split: Train={len(train_tensors)}, Val={len(val_tensors)}, Test={len(test_tensors)}")
  
      train_dataset = ImageDataset(tensor_folder, ab_grid, train_tensors)
      val_dataset = ImageDataset(tensor_folder, ab_grid, val_tensors)
      test_dataset = ImageDataset(tensor_folder, ab_grid, test_tensors)
  
      # SAVE TEST SET INFO TO PREVENT DATA LEAKAGE
      import json
      test_set_info = {
          'test_files': test_tensors,
          'num_bins': num_bin,
          'total_files': num_files,
          'train_size': len(train_tensors),
          'val_size': len(val_tensors),
          'test_size': len(test_tensors),
          'tensor_folder': tensor_folder
      }
      with open('/content/test_set_info.json', 'w') as f:
          json.dump(test_set_info, f)

      weights = class_weights(train_dataset, num_classes=len(ab_grid)).to(device)
  
      for hp in hps:
          lr, op, bs, wd = hp[0], hp[1], hp[2], hp[3]
          print(f"Training Model with LR: {lr}, OP: {op}, BS: {bs}, NE: {ne}, WD: {wd}")
  
          # Only use train/val for hyperparameter tuning
          train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, pin_memory=True, num_workers=2, persistent_workers=True)
          val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, pin_memory=True, num_workers=2, persistent_workers=True)
  
          model = ColorizationNet(num_classes=len(ab_grid))
          train(model, device, ab_grid, train_loader, val_loader, weights=weights, num_epochs=ne, lr=lr, op=op, wd=wd)
  
          visualize_predictions(model, val_loader, device, ab_grid_np)
  
          torch.cuda.empty_cache()
